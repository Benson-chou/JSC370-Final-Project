---
output: 
    html_document:
        toc: TRUE # table of content
        toc_float: TRUE
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
	message = FALSE, 
	warning=FALSE, 
	cache=TRUE,
	out.width = "80%", 
	fig.align = 'center'
)

library(kableExtra)
library(tidyverse)
library(jsonlite)
library(gridExtra)
library(rpart)
library(randomForest)
library(caret)
library(rpart.plot)
library(plotly)
```

# Methods
## Data Collection
As there are no API or directly downloadable data from Brilliant Earth, I scraped the data in Python, using Python's `Selenium Webdriver` to get the dynamic table's Json object, and using the `Json` and `pandas` packages to coerce this data into a data frame. 

The table shown below is an example of how the dataset looks like. 

```{r}
brilliant <- read.csv("data/brilliant.csv", header=TRUE)
head(brilliant, 5) %>% kable("html", caption="Diamond Table (Pre-cleaning)", booktabs = TRUE) %>% kable_styling(latex_options = c("scale_down", "HOLD_position"))
```

```{r}
variable <- c("price", "shape", "carat", "cut", "color", "clarity", "table", "depth")
descriptions <- c("Diamond's Price in CAD", "Shape of the diamond", "Weight of the diamond", "Quality of the cut", "Diamond colour, from J (worst) to D (best)", "A measurement of how clear the diamond is (SI2(worst), SI1, VS2, VS1, VVS2, VVS1, IF, FL(best))", "Width of top of diamond relative to widest point", "Total depth percentage of the diamond ")
var_des <- data.frame(Variables= variable, Descriptions = descriptions)
var_des %>% kable("html", caption="Data Descriptions", booktabs = TRUE) %>% kable_styling(latex_options = c("scale_down", "HOLD_position"))
```

(More information on table and depth can be found [here](https://www.diamonds.pro/education/diamond-depth-and-table/))

## Data Cleaning
The data collected seems to be well-formatted and the columns are already selected to be variables of interest during the web scraping process. However, we still need to clean the data for: missing data, duplicates, factorization, and unit. With the is.na() function, we know that there are no missing data. I removed the duplicate observations ith the distinct() function. I replaced cut values 'Super Ideal' into 'Premium' for simplicity and avoid confusion. Lastly, I used as.factor() to factorize the character variables.

```{r}
brilliant <- brilliant %>% distinct()
# For consistency, I will convert "Super Ideal" into "Premium"
brilliant$cut[brilliant$cut == 'Super Ideal'] <- 'Premium'
brilliant$cut <- factor(brilliant$cut, levels=c("Fair", "Good", "Very Good", "Ideal", "Premium"))
brilliant$shape <- as.factor(brilliant$shape)
brilliant$color <- as.factor(brilliant$color)
brilliant$clarity <- factor(brilliant$clarity, levels=c("SI2", "SI1", "VS2", "VS1", "VVS2", "VVS1", "IF", "FL"))
```

We can see that there are potential outliers from price, carat, table, and depth. Let's check if these observations were a mistake or not. 

```{r}
brilliant[brilliant$price == 23795.0 | brilliant$carat == 2.0100 | brilliant$table == 87.0 | brilliant$depth == 86.0,] %>% kable("html", caption="Potential Outliers", booktabs = TRUE) %>% kable_styling(latex_options = c("scale_down", "HOLD_position"))
```

We can see that even though these observations do not seem to be recorded as a mistake or placeholder for `NA` values, from the histogram below, these outliers affects our analysis and limits the conclusion we can make. Thus, I will remove the diamonds with prices that are considered as outliers (1.5IQR + Q3 and Q1 - 1.5IQR).
```{r}
pre <- brilliant %>% 
  ggplot(mapping=aes(x=price)) + 
  geom_histogram() + 
  labs(x="Price of Diamond (CAD)", title = "Diamond Prices")

new_brilliant <- brilliant[brilliant$price < 1343, ]

post <- new_brilliant %>% 
  ggplot(mapping=aes(x=price)) + 
  geom_histogram() + 
  labs(x="Price of Diamond (CAD)", title = "New Diamond Prices")

summary(new_brilliant) %>% kable("html", caption="Summary Table (Post Cleaning)", booktabs = TRUE) %>% kable_styling(latex_options = c("scale_down", "HOLD_position"))

grid.arrange(pre, post, ncol = 2)
```

Originally, we have 9793 diamond observations, and after filtering, we reduced to 9744 diamond observations

## Tools Used 
Data wrangling were completed with `tidyverse` and `dplyr`. Figures were created with `ggplot2`, interactive visuals were created using `plotly`. Tables were created with `kable` and `kableExtra`. Packages used for modelling include `rpart`, `randomForest`, `xgboost`. 

# Results 
## Summary Visuals
### Multi-variable Relationships

First, I will create boxplots between price and categorical variables such as cut, shape, color, and clarity.  
```{r}
price_cut <- new_brilliant %>% 
  ggplot(mapping=aes(x=cut, y=price)) +
  geom_boxplot() + 
  labs(title = "Prices for Each Diamond Cut")
price_shape <- new_brilliant %>% 
  ggplot(mapping=aes(x=shape, y=price)) +
  geom_boxplot() + 
  labs(title = "Prices for Each Diamond Shape") + 
  theme(axis.text.x = element_text(angle=90, hjust=1))
price_color <- new_brilliant %>% 
  ggplot(mapping=aes(x=color, y=price)) +
  geom_boxplot() + 
  labs(title = "Prices for Each Diamond Color")
price_clarity <- new_brilliant %>% 
  ggplot(mapping=aes(x=clarity, y=price)) +
  geom_boxplot() + 
  labs(title = "Prices for Each Diamond Clarity")
grid.arrange(price_cut, price_shape, price_color, price_clarity, ncol = 2, top="Price vs Categorical Variables")
```

We can see that for cut and clarity, while the price range for each level are about the same, which indicates a variety of options for each quality level, the median price increases as the quality gets better. However, we see an inverse relationship between color quality and price. For prices in each diamond shape, we don't see a clear pattern. However, we do see that for the more commonly seen shape (Round), it has a wider price range with a median of about 850-900 CAD. 

Next, I will create a general graph to show the relationship between carat and price for each cut and shape. 
```{r fig.height=8, fig.width=10}
cbp1 <- c("#999999", "#E69F00", "#56B4E9", "#009E73",
          "#F0E442", "#0072B2", "#D55E00", "#CC79A7")
fig <- new_brilliant %>% 
  ggplot(aes(jitter(carat), y=jitter(price), color = cut)) + 
  geom_point(alpha = 0.4, shape=21) + 
  geom_smooth(method = 'lm', se=FALSE, size = 0.8, color = "red") + 
  facet_wrap(vars(shape)) + 
  labs(title = "Carat vs Price for Each Cut Quality for Each Shape", x="Carat", y="Price") + scale_colour_manual(values=cbp1)
ggplotly(fig)
```

We also see a similar trend among different groups, which is as carat increases, the expected diamond price also increases. 

Next, I will create scatterplot over price vs table and depth.
```{r fig.height=10, fig.width=12}
lm_d = lm(price~depth, new_brilliant)
lm_t = lm(price~table, new_brilliant)

p2 <- plot_ly(new_brilliant, y = ~jitter(price)) %>% 
  add_trace(x = ~depth, name = "depth", mode = "markers", opacity=0.5) %>%
  add_trace(x = ~table, name = "table", mode = "markers", opacity=0.5) %>% 
  layout(xaxis = list(title = "Percentage"), title = list(title = "Depth and Table vs Price"), yaxis = list(title = "Price")) %>% 
  add_lines(x = ~depth, y = fitted(lm_d), name="Depth Trend", line = list(color="blue")) %>% 
  add_lines(x = ~table, y = fitted(lm_t), name="Table Trend", line = list(color="orange"))
p2
```

We see that despite having diamonds with all ranges of table and depth at each price level, table does not seem to have a positive relationship with price, while the depth percentage has a positive relationship. 

## Modelling
### Linear Model

I tried fitting a basic linear model with all the variables as price is a continuous variable that can be predicted by a linear model. 
```{r}
model1 <- lm(price~., data=new_brilliant)
lm_sum <- summary(model1)
```

This model has an `r lm_sum$adj.r.squared * 100`% which means that only about `r lm_sum$adj.r.squared * 100`% of the variance of our predictors is explained by the variance of price. Notice that the insignificant variables are: `shapeCushion`, `shapeEmerald`, `shapePrincess`, `shapeRadiant`, `cutGood`, `cutVery Good` based on their p-values from Wald Z-test:
```{r}
sum_tab <- as.data.frame(lm_sum$coefficients[, c(1, 4)])
colnames(sum_tab)[1] <- "Estimate"
colnames(sum_tab)[2] <- "p-values"
sum_tab <- sum_tab %>% mutate_if(is.numeric, ~round(., 3))
sum_tab %>% kable("html", longtable = T, booktabs = TRUE) %>% kable_styling(latex_options = c("scale_down", "HOLD_position", "repeat_header"))
```

With that in mind, I attempted to see if removing these insignificant factors will yield a better $R^2$. 

This new model yields an $R^2$ of 0.48632, which is lower than the original model. Thus, we would want to take the full model rather than the reduced model. 

### Interpretation of Values

The estimates above represent the change in prices of a diamond for a unit increase in the continuous variables or for being a certain level of a categorical variable. 

The estimated diamond price is about -351.7 CAD when all other variables are 0. Only looking at the variables with significant p-values, we see the most influential variable being carat, with an increase of 2211.366 CAD for each unit increase in carat. We also see that with the diamond having a clarity of IF (the second highest clarity), it increases the price by about 237.615 CAD. Surprisingly, our model predicts that no matter which color a diamond belongs to, it decreases the price. 


## Machine Learning Models

Next, I will try machine learning models such as Decision Tree, Random Forest, and XGBoost to further explore which variables are important when predicting a diamond price. RMSE will be used to compare model performance. 

### Decision Tree

I fit a decision tree to see what qualities can really separate the price level of a diamond. In order for the graph to be comprehensible and fit for the general cases, I set the max tree depth to 15, and pruned the tree at the minimum x-error, which has a cp-value of 0.0003427563. 
```{r}
set.seed(50)
n <- nrow(new_brilliant)
train_size <- sample(n, 0.8*n)
new_bril_train <- new_brilliant[train_size, ]
new_bril_test <- new_brilliant[-train_size, ]
```


```{r}
tree <- rpart(price~., data=new_bril_train, method="anova", 
      control = list(cp=0), maxdepth = 10)
```
```{r, include=FALSE, echo=FALSE}
cp_summary <- printcp(tree)
```

```{r}
optimal_cp <- cp_summary[90, 1]
tree_pruned <- prune(tree, cp = optimal_cp)
rpart.plot(tree_pruned)
```

Within each tree node, we have the predicted price and the percentage of that quality with the predicted price. 
```{r}
dt_imp <- as.data.frame(tree_pruned$variable.importance)
colnames(dt_imp)[1] <- "Variable Importance"
dt_imp %>% kable("html", caption="Decision Tree Variable Importance", booktabs = TRUE) %>% kable_styling(latex_options = c("HOLD_position"))
```

### Random Forest

As the model attempts different value combinations, I didn't further specify more hyperparameters. 
```{r}
n_features <- dim(new_brilliant)[2] - 1

brill_bagging <- randomForest(price ~., 
                              data=new_bril_train, 
                              mtry=n_features, 
                              na.action = na.omit)
rf_Imp <- as.data.frame(importance(brill_bagging)) %>% arrange(desc(IncNodePurity))
colnames(rf_Imp)[1] <- "Variable Importance"
rf_Imp %>% kable("html", caption="Random Forest Variable Importance", booktabs = TRUE) %>% kable_styling(latex_options = c("HOLD_position"))
```

### XGBoost

XGBoost tunes the max depth, number of estimators, and learning rate for the optimal performance. The final model has the parameters: `nrounds = 300, max_depth = 5, eta = 0.1`
```{r}
train_control <- caret::trainControl(method = "cv", number = 10, search = "grid")

tune_grid <- expand.grid(
  max_depth = c(1, 3, 5, 10),
  nrounds = 50*(1:10),
  eta = c(.3, .1, .01, .001),
  gamma = 0,
  colsample_bytree = .6,
  subsample = 1,
  min_child_weight = 1
)

bril_xgb <- caret::train(
  price ~.,
  data = new_bril_train,
  method = "xgbTree",
  trControl = train_control,
  tuneGrid = tune_grid,
  verbosity = 0.
)
xgb_imp <- varImp(bril_xgb, scale = FALSE)
xgb_imp <- as.data.frame(xgb_imp$importance)
xgb_imp %>% kable("html", caption="XGBoost Variable Importance", booktabs = TRUE) %>% kable_styling(latex_options = c("HOLD_position"))
```


To find out which model performed the best, we will look for the model with the lowest RMSE. As we see from the table below, XGBoost yielded the best results out of the three. 
```{r}
# RMSE
tree_pred <- predict(tree_pruned, new_bril_test)
rmse_tree <- mean((new_bril_test$price - tree_pred)^2) %>% sqrt() 

pred_rf <- predict(brill_bagging, new_bril_test)
rmse_rf <- mean((new_bril_test$price - pred_rf)^2) %>% sqrt()

yhat_xgb <- predict(bril_xgb, newdata = new_bril_test, type = "raw")
yhat_xgb <- as.numeric(yhat_xgb) - 1
rmse_xgb <- caret::RMSE(new_bril_test$price, yhat_xgb)

model <- c("Decision Tree", "Random Forest", "XGBoost")
values <- c(rmse_tree, rmse_rf, rmse_xgb)
rmse_df <- data.frame(Model = model, RMSE = values)
rmse_df %>% kable("html", caption="RMSE for Each Model", booktabs = TRUE) %>% kable_styling(latex_options = c("HOLD_position"))
```

As the RMSE's are still within reasonable range, we can take the variable importance outputs and the results from the linear model for a clear idea of the relationship between price and our predictors.
